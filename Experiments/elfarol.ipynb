{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0f6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3de08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k3n\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002825A612CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002825A613F60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_AGENTS = 100\n",
    "BAR_CAPACITY = 60\n",
    "WEEKS = 100\n",
    "SEQUENCE_LENGTH = 5\n",
    "\n",
    "# Generate dummy attendance history\n",
    "def generate_initial_attendance(weeks=SEQUENCE_LENGTH):\n",
    "    return [random.randint(40, 70) for _ in range(weeks)]\n",
    "\n",
    "# Build LSTM predictor\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(10, input_shape=(SEQUENCE_LENGTH, 1), return_sequences=False),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Simple Q-learning Agent\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((2,))  # 0: stay home, 1: go\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.2\n",
    "        self.lstm = build_lstm_model()\n",
    "        self.past_attendance = generate_initial_attendance()\n",
    "        # Train the LSTM model with the dummy data\n",
    "        X = np.array([self.past_attendance[i:i+SEQUENCE_LENGTH] for i in range(len(self.past_attendance) - SEQUENCE_LENGTH)])\n",
    "        y = np.array(self.past_attendance[SEQUENCE_LENGTH:])\n",
    "        if len(X) > 0:\n",
    "            self.lstm.fit(X.reshape(-1, SEQUENCE_LENGTH, 1), y, epochs=20, verbose=0)\n",
    "\n",
    "    def predict_attendance(self):\n",
    "        sequence = np.array(self.past_attendance[-SEQUENCE_LENGTH:]).reshape(1, SEQUENCE_LENGTH, 1)\n",
    "        prediction = self.lstm.predict(sequence, verbose=0)[0][0]\n",
    "        return prediction\n",
    "\n",
    "    def decide(self, predicted_attendance):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice([0, 1])\n",
    "        else:\n",
    "            return np.argmax(self.q_table)\n",
    "\n",
    "    def update_q(self, action, reward):\n",
    "        self.q_table[action] = self.q_table[action] + self.alpha * (reward + self.gamma * np.max(self.q_table) - self.q_table[action])\n",
    "\n",
    "    def update_attendance_history(self, value):\n",
    "        self.past_attendance.append(value)\n",
    "        if len(self.past_attendance) > SEQUENCE_LENGTH + 1:\n",
    "            self.past_attendance.pop(0)\n",
    "\n",
    "# Initialize agents\n",
    "agents = [Agent() for _ in range(N_AGENTS)]\n",
    "\n",
    "# Simulation\n",
    "history = generate_initial_attendance(weeks=SEQUENCE_LENGTH)\n",
    "for week in range(WEEKS):\n",
    "    decisions = []\n",
    "    predictions = []\n",
    "    for agent in agents:\n",
    "        pred = agent.predict_attendance()\n",
    "        predictions.append(pred)\n",
    "        action = agent.decide(pred)\n",
    "        decisions.append(action)\n",
    "    \n",
    "    attendance = sum(decisions)\n",
    "    history.append(attendance)\n",
    "    history = history[-(SEQUENCE_LENGTH + 1):]\n",
    "\n",
    "    for i, agent in enumerate(agents):\n",
    "        reward = 1 if (decisions[i] == 1 and attendance <= BAR_CAPACITY) else -1 if decisions[i] == 1 else 0.5\n",
    "        agent.update_q(decisions[i], reward)\n",
    "        agent.update_attendance_history(attendance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4da0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAR_CAPACITY: 60\n",
      "N_AGENTS: 100\n",
      "SEQUENCE_LENGTH: 5\n",
      "WEEKS: 100\n",
      "Current Week: 99\n",
      "Attendance: 31\n",
      "Decisions: [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "Predictions: [-0.040037114, -0.80520296, -0.4072002, 0.75555164, 0.29798466, -0.11574817, -0.4437642, 0.6173918, -1.2909397, -0.59182274, -1.2068125, 0.14483353, 1.1339676, 0.6434838, -1.9307551, 0.12221027, 0.29612422, -0.5775117, 0.0979374, -0.5286367, -1.0675871, -0.5562646, 0.5063758, -0.43042237, 0.60359734, 0.3607213, -0.6842465, 0.20143822, 0.6790423, 0.85435593, 0.6081389, 0.40812677, -0.1602152, 0.19909333, 0.51742697, 1.5107089, 0.2864718, -0.06613885, 0.21032326, -1.3621294, 0.9226646, 0.10392186, 0.20023642, -0.20538048, 0.32023907, -0.08152153, 0.3980237, -0.063250095, 0.23754537, -1.218721, -0.21465816, 1.073738, -0.23120387, 0.24651065, -0.6349676, -0.15471959, 0.054280255, -1.746536, -0.35226637, 0.16299358, 0.29921156, 1.0125595, -0.29381865, -0.024827925, 0.8767672, -0.20228966, -2.207237, -1.1231964, -0.49811575, 1.6706934, -0.39395362, -0.30655867, -0.07355057, 1.5582963, 0.9103991, -0.33570918, -0.33511963, -0.49979275, -0.28087416, -0.34655932, 0.51429445, 0.47934744, -0.20235682, 0.26147646, 0.12124536, -0.20520075, -0.39089814, 0.5640567, 0.38008475, -0.61753714, 0.34799054, -0.28958502, -0.50376284, 0.29864267, 1.1718841, -0.011879159, -0.20610131, 0.4345696, -0.49196357, -0.34034023]\n",
      "History: [32, 31, 31, 34, 29, 31]\n",
      "Reward: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"BAR_CAPACITY:\", BAR_CAPACITY)\n",
    "print(\"N_AGENTS:\", N_AGENTS)\n",
    "print(\"SEQUENCE_LENGTH:\", SEQUENCE_LENGTH)\n",
    "print(\"WEEKS:\", WEEKS)\n",
    "print(\"Current Week:\", week)\n",
    "print(\"Attendance:\", attendance)\n",
    "print(\"Decisions:\", decisions)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"History:\", history)\n",
    "print(\"Reward:\", reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
